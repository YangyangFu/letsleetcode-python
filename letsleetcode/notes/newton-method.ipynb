{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newton Iterative Method\n",
    "\n",
    "Newton itertative method is typically used to find root of a given equality and to find a minimum of a convex and twice-differentiable function.\n",
    "\n",
    "For finding the root of $f(x) = 0$, the Newton method update root $x$ using\n",
    "\n",
    "$x_{k+1} = x_k - t\\frac{f(x_k)}{\\nabla f(x_k)}$\n",
    "\n",
    "\n",
    "For minimizing an unconstrainted convex function $f(x)$, the Newton method update $x$ using\n",
    "\n",
    "$x_{k+1} = x_k - t\\frac{\\nabla f(x_k)}{\\nabla^2 f(x_k)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Finding\n",
    "\n",
    "The Newton method can be derived and proved from Taylor expansion theory, which approximates $f(x)$ at $x$ with different orders of truncating errors. \n",
    "\n",
    "Let $x^*$ be the optimal solution of a root finding problem as follows:\n",
    "\n",
    "$$\n",
    "    f(x) = 0 \\tag{1}\n",
    "$$\n",
    "\n",
    "\n",
    "The first-order approximation of the optimality $f(x^*)$ at $x$, would be:\n",
    "\n",
    "$$\n",
    "    f(x^*) = f(x) + \\nabla f(x)(x^* - x) \\tag{2}\n",
    "$$\n",
    "\n",
    "Since $x^*$ is a solution to Eqn.(1), Eqn.(2) can be rewritten as:\n",
    "\n",
    "$$\n",
    "    0 = f(x) + \\nabla f(x)(x^* - x) \\tag{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "    x^* = x - \\frac{f(x)}{\\nabla f(x)} \\tag{4}\n",
    "$$\n",
    "\n",
    "Therefore the above update rules can be used to find the optomal solution given tolerances. Numerically, a step size factor $t$ is typically added as follows.\n",
    "\n",
    "$$\n",
    "    x_{k+1} = x_k - t\\frac{f(x_k)}{\\nabla f(x_k)} \\tag{5}\n",
    "$$\n",
    "\n",
    "The stoping crietia can be: \n",
    "\n",
    "$$\n",
    "    f(x_k) \\le \\epsilon \\tag{6}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unconstrained Minimization Probelm\n",
    "\n",
    "Here we want to find the optimal solution of a convex and twice-differentiable function $g(x)$ as follows:\n",
    "\n",
    "$$ \n",
    "    \\min_x g(x) \\tag{6}\n",
    "$$\n",
    "\n",
    "Applying the Newton's method for this optimziation problem is the same as find the root of $\\nabla g(x) = 0$ as the gradient vanishes at the optimal point of a convex and differentiable function.\n",
    "\n",
    "Let $f(x) = \\nabla g(x)$, then we have \n",
    "\n",
    "\n",
    "$$\n",
    "    x_{k+1} = x_k - t\\frac{f(x_k)}{\\nabla f(x_k)} = x_k -t \\frac{\\nabla g(x_k)}{\\nabla^2 g(x_k)} \\tag{7}\n",
    "$$\n",
    "\n",
    "### Newton Step\n",
    "\n",
    "Newton step is known as:\n",
    "\n",
    "$$\n",
    "    \\Delta x_n = -\\frac{\\nabla g(x_k)}{\\nabla^2 g(x_k)} \\tag{8}\n",
    "$$\n",
    "\n",
    "### Newton Decrement\n",
    "\n",
    "Define Newton decrement as \n",
    "\n",
    "$$\n",
    "    \\lambda(x) = (\\nabla g(x)^T\\nabla^2 g(x)^{-1}\\nabla g(x))^{\\frac{1}{2}} =  (\\Delta x_n^T\\nabla^2 g(x)\\Delta x_n)^{\\frac{1}{2}} \\tag{8}\n",
    "$$\n",
    "\n",
    "### Stopping Crietia\n",
    "\n",
    "Using Newton's updating rule to update $x$ at each step, we can get the second-order approximation of the funcation $g$ at $x$:\n",
    "\n",
    "$$\n",
    "    g(x_{k+1}) = g(x_k)  + \\nabla g(x_k)\\Delta x_n +\\frac{1}{2} \\Delta x_n \\nabla^2 g(x_k) \\Delta x_n \\tag{8}\n",
    "$$\n",
    "\n",
    "Thus the function evaluation error between steps is:\n",
    "\n",
    "$$\n",
    "    g(x_{k+1}) - g(x_k)  = \\nabla g(x_k)\\Delta x_n +\\frac{1}{2} \\Delta x_n \\nabla^2 g(x_k) \\Delta x_n = -\\frac{1}{2}\\lambda(x_k)^2\n",
    "$$\n",
    "\n",
    "Therefore controlling Newton decrement can help stop the iterations.\n",
    "\n",
    "$$\n",
    "    \\frac{1}{2}\\lambda(x_k)^2 <= \\epsilon\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Equality Constrained Minimization Problem\n",
    "\n",
    "Newton's method can also be extended to solve the following equality constrained problem:\n",
    "\n",
    "$$\n",
    "\\min_x  \\ f(x) \\\\\n",
    "\\text{s.t.} \\ Ax=b \\tag{9}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
